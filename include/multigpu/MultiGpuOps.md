<!-- Autogenerated by mlir-tblgen; don't manually edit -->
### `mgpu.all_gather` (multigpu::AllGatherOp)

_All-gather across all ranks in a communicator_


Syntax:

```
operation ::= `mgpu.all_gather` $sendBuf `,` $recvBuf `,` $comm
              (`stream` $stream^ `:` type($stream))?
              attr-dict `:` type($sendBuf) `,` type($recvBuf) `,` type($comm)
```

Each rank contributes `sendBuf`; every rank receives the concatenation of
all contributions into `recvBuf`. The leading dimension of `recvBuf` must
equal N x the corresponding dimension of `sendBuf`, where N is the number
of ranks. Element types must match.

Traits: `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `sendBuf` | memref of any type values
| `recvBuf` | memref of any type values
| `comm` | communicator handle
| `stream` | stream handle

### `mgpu.all_reduce` (multigpu::AllReduceOp)

_All-reduce across all ranks in a communicator_


Syntax:

```
operation ::= `mgpu.all_reduce` $sendBuf `,` $recvBuf `,` $comm `,` $op
              (`stream` $stream^ `:` type($stream))?
              attr-dict `:` type($sendBuf) `,` type($recvBuf) `,` type($comm)
```

Each rank contributes `sendBuf` and receives the element-wise reduction
into `recvBuf`. `op` names the reduction operation (e.g. "sum", "max",
"min", "prod"). Send and receive buffers must have the same type; in-place
reduction (sendBuf == recvBuf) is permitted.

Traits: `RecursiveMemoryEffects`

#### Attributes:

<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>op</code></td><td>::mlir::StringAttr</td><td>string attribute</td></tr>
</table>

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `sendBuf` | memref of any type values
| `recvBuf` | memref of any type values
| `comm` | communicator handle
| `stream` | stream handle

### `mgpu.alloc` (multigpu::AllocOp)

_Allocate device memory_


Syntax:

```
operation ::= `mgpu.alloc` $device attr-dict `:` type($device) `->` type($ptr)
```

Allocates memory on the given device. The shape and element type of the
result memref fully determine the allocation size.

Traits: `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `device` | device handle

#### Results:

| Result | Description |
| :----: | ----------- |
| `ptr` | memref of any type values

### `mgpu.broadcast` (multigpu::BroadcastOp)

_Broadcast from a root rank to all ranks_


Syntax:

```
operation ::= `mgpu.broadcast` $buf `,` $comm `,` $rootRank
              (`stream` $stream^ `:` type($stream))?
              attr-dict `:` type($buf) `,` type($comm)
```

The root rank's `buf` is copied to every other rank's `buf`. All ranks
must call this op with the same root and buffer shape.

Traits: `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `buf` | memref of any type values
| `comm` | communicator handle
| `rootRank` | index
| `stream` | stream handle

### `mgpu.create_communicator` (multigpu::CreateCommunicatorOp)

_Create a communicator over a set of devices_


Syntax:

```
operation ::= `mgpu.create_communicator` $devices attr-dict `:` type($devices) `->` type($comm)
```

The device list must contain at least one device and must not contain
duplicates. The ordering of devices determines each device's rank within
the communicator.

Traits: `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `devices` | device handle

#### Results:

| Result | Description |
| :----: | ----------- |
| `comm` | communicator handle

### `mgpu.create_stream` (multigpu::CreateStreamOp)

_Create a new stream on a device_


Syntax:

```
operation ::= `mgpu.create_stream` $device attr-dict `:` type($device) `->` type($stream)
```


Traits: `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `device` | device handle

#### Results:

| Result | Description |
| :----: | ----------- |
| `stream` | stream handle

### `mgpu.destroy_stream` (multigpu::DestroyStreamOp)

_Destroy a stream_


Syntax:

```
operation ::= `mgpu.destroy_stream` $stream attr-dict `:` type($stream)
```


Traits: `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `stream` | stream handle

### `mgpu.free` (multigpu::FreeOp)

_Free device memory_


Syntax:

```
operation ::= `mgpu.free` $ptr `,` $device attr-dict `:` type($ptr) `,` type($device)
```


Traits: `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `ptr` | memref of any type values
| `device` | device handle

### `mgpu.get_device` (multigpu::GetDeviceOp)

_Get a handle to a GPU device by index_


Syntax:

```
operation ::= `mgpu.get_device` $index attr-dict `:` type($device)
```

Returns a handle to the device at the given index. The index must be in
range [0, number_of_devices). Behavior is undefined otherwise.

Traits: `AlwaysSpeculatableImplTrait`

Interfaces: `ConditionallySpeculatable`, `NoMemoryEffect (MemoryEffectOpInterface)`

Effects: `MemoryEffects::Effect{}`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `index` | index

#### Results:

| Result | Description |
| :----: | ----------- |
| `device` | device handle

### `mgpu.launch` (multigpu::LaunchOp)

_Launch a kernel on a device_

Launches the single-block region as a kernel on the given device.

`grid` specifies the number of blocks in each dimension (up to 3D).
`block` specifies the number of threads per block in each dimension
(up to 3D). Both must have the same length (1-3). Operand segment sizes
are recorded by the `AttrSizedOperandSegments` trait.

If `stream` is provided the kernel is enqueued on that stream (async);
otherwise it executes synchronously.

Traits: `AttrSizedOperandSegments`, `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `device` | device handle
| `grid` | index
| `block` | index
| `stream` | stream handle

### `mgpu.memcpy` (multigpu::MemcpyOp)

_Copy memory between devices_


Syntax:

```
operation ::= `mgpu.memcpy` $dst `,` $src `,` $dstDevice `,` $srcDevice
              (`stream` $stream^ `:` type($stream))?
              attr-dict `:` type($dst) `,` type($src) `,` type($dstDevice) `,` type($srcDevice)
```

Copies the contents of `src` to `dst`. The source and destination memrefs
must have the same element type and total number of elements; this is
verified.

If `stream` is provided the copy is enqueued on that stream (async);
otherwise it executes synchronously.

Traits: `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `dst` | memref of any type values
| `src` | memref of any type values
| `dstDevice` | device handle
| `srcDevice` | device handle
| `stream` | stream handle

### `mgpu.recv` (multigpu::RecvOp)

_Point-to-point receive_


Syntax:

```
operation ::= `mgpu.recv` $buf `,` $comm `,` $srcRank
              (`stream` $stream^ `:` type($stream))?
              attr-dict `:` type($buf) `,` type($comm)
```


Traits: `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `buf` | memref of any type values
| `comm` | communicator handle
| `srcRank` | index
| `stream` | stream handle

### `mgpu.reduce_scatter` (multigpu::ReduceScatterOp)

_Reduce-scatter across all ranks in a communicator_


Syntax:

```
operation ::= `mgpu.reduce_scatter` $sendBuf `,` $recvBuf `,` $comm `,` $op
              (`stream` $stream^ `:` type($stream))?
              attr-dict `:` type($sendBuf) `,` type($recvBuf) `,` type($comm)
```

The inverse of AllGatherOp. Each rank contributes `sendBuf`; the
element-wise reduction is computed and the result is scattered so that
rank i receives the i-th slice into `recvBuf`. `op` names the reduction.

Traits: `RecursiveMemoryEffects`

#### Attributes:

<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>op</code></td><td>::mlir::StringAttr</td><td>string attribute</td></tr>
</table>

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `sendBuf` | memref of any type values
| `recvBuf` | memref of any type values
| `comm` | communicator handle
| `stream` | stream handle

### `mgpu.send` (multigpu::SendOp)

_Point-to-point send_


Syntax:

```
operation ::= `mgpu.send` $buf `,` $comm `,` $dstRank
              (`stream` $stream^ `:` type($stream))?
              attr-dict `:` type($buf) `,` type($comm)
```


Traits: `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `buf` | memref of any type values
| `comm` | communicator handle
| `dstRank` | index
| `stream` | stream handle

### `mgpu.stream_wait` (multigpu::StreamWaitOp)

_Make one stream wait on another_


Syntax:

```
operation ::= `mgpu.stream_wait` $waitingStream `,` $waitOnStreams attr-dict `:` type($waitingStream) `,` type($waitOnStreams)
```

Records the current state of each stream in `waitOnStreams` and makes
`waitingStream` wait on all of them. Work submitted to `waitingStream` after
this op will not begin until all recorded work on the waited-on streams has
completed. The waiting stream and the waited-on streams may reside on
different devices (cross-device stream synchronization).

Traits: `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `waitingStream` | stream handle
| `waitOnStreams` | stream handle

### `mgpu.sync_device` (multigpu::SyncDeviceOp)

_Synchronize all streams on a device_


Syntax:

```
operation ::= `mgpu.sync_device` $device attr-dict `:` type($device)
```

Blocks the host until all work submitted to any stream on the given device
has completed.

Traits: `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `device` | device handle

### `mgpu.sync_stream` (multigpu::SyncStreamOp)

_Synchronize a single stream_


Syntax:

```
operation ::= `mgpu.sync_stream` $stream attr-dict `:` type($stream)
```

Blocks the host until all work submitted to the given stream has completed.

Traits: `RecursiveMemoryEffects`

#### Operands:

| Operand | Description |
| :-----: | ----------- |
| `stream` | stream handle

### `mgpu.terminator` (multigpu::TerminatorOp)

_Terminator for mgpu launch regions._


Syntax:

```
operation ::= `mgpu.terminator` attr-dict
```

A terminator operation for regions that appear in the body of `mgpu.launch`
operation. These regions are not expected to return any value so the
terminator takes no operands.

Traits: `AlwaysSpeculatableImplTrait`, `HasParent<LaunchOp>`, `Terminator`

Interfaces: `ConditionallySpeculatable`, `NoMemoryEffect (MemoryEffectOpInterface)`

Effects: `MemoryEffects::Effect{}`

