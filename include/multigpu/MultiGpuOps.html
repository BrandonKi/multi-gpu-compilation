<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang xml:lang>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>MultiGPU Dialect Operations</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <style type="text/css">

html {
font-size: 62.5%;
font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif;
}
body {
font-size: 1.8rem;
line-height: 1.618;
max-width: 38em;
margin: auto;
color: #4a4a4a;
background-color: #f9f9f9;
padding: 13px;
}
@media (max-width: 684px) {
body {
font-size: 1.53rem;
}
}
@media (max-width: 382px) {
body {
font-size: 1.35rem;
}
}
h1, h2, h3, h4, h5, h6 {
line-height: 1.1;
font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif;
font-weight: 700;
margin-top: 3rem;
margin-bottom: 1.5rem;
overflow-wrap: break-word;
word-wrap: break-word;
-ms-word-break: break-all;
word-break: break-word;
}
h1 {
font-size: 2.35em;
}
h2 {
font-size: 2em;
}
h3 {
font-size: 1.75em;
}
h4 {
font-size: 1.5em;
}
h5 {
font-size: 1.25em;
}
h6 {
font-size: 1em;
}
p {
margin-top: 0px;
margin-bottom: 2.5rem;
}
small, sub, sup {
font-size: 75%;
}
hr {
border-color: #1d7484;
}
a {
text-decoration: none;
color: #1d7484;
}
a:visited {
color: #144f5a;
}
a:hover {
color: #982c61;
border-bottom: 2px solid #4a4a4a;
}
ul {
padding-left: 1.4em;
margin-top: 0px;
margin-bottom: 2.5rem;
}
li {
margin-bottom: 0.4em;
}
blockquote {
margin-left: 0px;
margin-right: 0px;
padding-left: 1em;
padding-top: 0.8em;
padding-bottom: 0.8em;
padding-right: 0.8em;
border-left: 5px solid #1d7484;
margin-bottom: 2.5rem;
background-color: #f1f1f1;
}
blockquote p {
margin-bottom: 0;
}
img, video {
height: auto;
max-width: 100%;
margin-top: 0px;
margin-bottom: 2.5rem;
}

pre {
background-color: #f1f1f1;
display: block;
padding: 1em;
overflow-x: auto;
margin-top: 0px;
margin-bottom: 2.5rem;
font-size: 0.9em;
}
code, kbd, samp {
font-size: 0.9em;
padding: 0 0.5em;
background-color: #f1f1f1;
white-space: pre-wrap;
}
pre > code {
padding: 0;
background-color: transparent;
white-space: pre;
font-size: 1em;
}

table {
text-align: justify;
width: 100%;
border-collapse: collapse;
margin-bottom: 2rem;
}
td, th {
padding: 0.5em;
border-bottom: 1px solid #f1f1f1;
}

input, textarea {
border: 1px solid #4a4a4a;
}
input:focus, textarea:focus {
border: 1px solid #1d7484;
}
textarea {
width: 100%;
}
.button, button,
input[type=submit],
input[type=reset],
input[type=button],
input[type=file]::file-selector-button {
display: inline-block;
padding: 5px 10px;
text-align: center;
text-decoration: none;
white-space: nowrap;
background-color: #1d7484;
color: #f9f9f9;
border-radius: 1px;
border: 1px solid #1d7484;
cursor: pointer;
box-sizing: border-box;
}
.button:hover, button:hover,
input[type=submit]:hover,
input[type=reset]:hover,
input[type=button]:hover,
input[type=file]::file-selector-button:hover {
background-color: #982c61;
color: #f9f9f9;
outline: 0;
}
.button[disabled], button[disabled],
input[type=submit][disabled],
input[type=reset][disabled],
input[type=button][disabled],
input[type=file][disabled] {
cursor: default;
opacity: 0.5;
}
.button:focus-visible, button:focus-visible,
input[type=submit]:focus-visible,
input[type=reset]:focus-visible,
input[type=button]:focus-visible,
input[type=file]:focus-visible {
outline-style: solid;
outline-width: 2px;
}
textarea, select, input {
color: #4a4a4a;
padding: 6px 10px; 
margin-bottom: 10px;
background-color: #f1f1f1;
border: 1px solid #f1f1f1;
border-radius: 4px;
box-shadow: none;
box-sizing: border-box;
}
textarea:focus, select:focus, input:focus {
border: 1px solid #1d7484;
outline: 0;
}
input[type=checkbox]:focus {
outline: 1px dotted #1d7484;
}
label, legend, fieldset {
display: block;
margin-bottom: 0.5rem;
font-weight: 600;
}
</style>
  <style type="text/css">
header { border-bottom: 2px solid #eee; margin-bottom: 2em; }
h1.title { font-weight: bold;
border: none; }
@media (min-width: 1200px) {
#TOC {
position: fixed !important;
left: 20px !important; top: 20px !important;
width: 250px !important;
height: 90vh !important;
overflow-y: auto !important;
font-size: 0.8em !important;
border-right: 1px solid #eee !important;
padding-right: 10px !important;
}
body {
margin-left: 300px !important;
max-width: 800px !important;
}
}
</style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">MultiGPU Dialect Operations</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#mgpu-dialect">‘mgpu’ Dialect</a>
<ul>
<li><a href="#operations">Operations</a>
<ul>
<li><a href="#mgpu.all_gather-multigpuallgatherop"><code>mgpu.all_gather</code> (multigpu::AllGatherOp)</a></li>
<li><a href="#mgpu.all_reduce-multigpuallreduceop"><code>mgpu.all_reduce</code> (multigpu::AllReduceOp)</a></li>
<li><a href="#mgpu.alloc-multigpuallocop"><code>mgpu.alloc</code> (multigpu::AllocOp)</a></li>
<li><a href="#mgpu.broadcast-multigpubroadcastop"><code>mgpu.broadcast</code> (multigpu::BroadcastOp)</a></li>
<li><a href="#mgpu.create_communicator-multigpucreatecommunicatorop"><code>mgpu.create_communicator</code> (multigpu::CreateCommunicatorOp)</a></li>
<li><a href="#mgpu.create_stream-multigpucreatestreamop"><code>mgpu.create_stream</code> (multigpu::CreateStreamOp)</a></li>
<li><a href="#mgpu.destroy_stream-multigpudestroystreamop"><code>mgpu.destroy_stream</code> (multigpu::DestroyStreamOp)</a></li>
<li><a href="#mgpu.device_config-multigpudeviceconfigop"><code>mgpu.device_config</code> (multigpu::DeviceConfigOp)</a></li>
<li><a href="#mgpu.free-multigpufreeop"><code>mgpu.free</code> (multigpu::FreeOp)</a></li>
<li><a href="#mgpu.gather-multigpugatherop"><code>mgpu.gather</code> (multigpu::GatherOp)</a></li>
<li><a href="#mgpu.get_device-multigpugetdeviceop"><code>mgpu.get_device</code> (multigpu::GetDeviceOp)</a></li>
<li><a href="#mgpu.launch-multigpulaunchop"><code>mgpu.launch</code> (multigpu::LaunchOp)</a></li>
<li><a href="#mgpu.memcpy-multigpumemcpyop"><code>mgpu.memcpy</code> (multigpu::MemcpyOp)</a></li>
<li><a href="#mgpu.recv-multigpurecvop"><code>mgpu.recv</code> (multigpu::RecvOp)</a></li>
<li><a href="#mgpu.replicate-multigpureplicateop"><code>mgpu.replicate</code> (multigpu::ReplicateOp)</a></li>
<li><a href="#mgpu.scatter-multigpuscatterop"><code>mgpu.scatter</code> (multigpu::ScatterOp)</a></li>
<li><a href="#mgpu.send-multigpusendop"><code>mgpu.send</code> (multigpu::SendOp)</a></li>
<li><a href="#mgpu.sharded_alloc-multigpushardedallocop"><code>mgpu.sharded_alloc</code> (multigpu::ShardedAllocOp)</a></li>
<li><a href="#mgpu.sharded_free-multigpushardedfreeop"><code>mgpu.sharded_free</code> (multigpu::ShardedFreeOp)</a></li>
<li><a href="#mgpu.stream_wait-multigpustreamwaitop"><code>mgpu.stream_wait</code> (multigpu::StreamWaitOp)</a></li>
<li><a href="#mgpu.sync_device-multigpusyncdeviceop"><code>mgpu.sync_device</code> (multigpu::SyncDeviceOp)</a></li>
<li><a href="#mgpu.sync_stream-multigpusyncstreamop"><code>mgpu.sync_stream</code> (multigpu::SyncStreamOp)</a></li>
<li><a href="#mgpu.terminator-multigputerminatorop"><code>mgpu.terminator</code> (multigpu::TerminatorOp)</a></li>
<li><a href="#mgpu.uneven_scatter-multigpuunevenscatterop"><code>mgpu.uneven_scatter</code> (multigpu::UnevenScatterOp)</a></li>
</ul></li>
<li><a href="#attributes-6">Attributes</a>
<ul>
<li><a href="#deviceconfigattr">DeviceConfigAttr</a></li>
</ul></li>
<li><a href="#types">Types</a>
<ul>
<li><a href="#communicatortype">CommunicatorType</a></li>
<li><a href="#devicetype">DeviceType</a></li>
<li><a href="#streamtype">StreamType</a></li>
</ul></li>
<li><a href="#enums">Enums</a>
<ul>
<li><a href="#reductionkind">ReductionKind</a></li>
<li><a href="#shardkind">ShardKind</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
<!-- Autogenerated by mlir-tblgen; don't manually edit -->
<h1 id="mgpu-dialect">‘mgpu’ Dialect</h1>
<p>The <code>mgpu</code> dialect models high-level multi-GPU computation. This includes memory management, kernel launches, device synchronization, and collective communication primitives. It uses a stream-based async execution model for parallelism.</p>
<p>[TOC]</p>
<h2 id="operations">Operations</h2>
<h3 id="mgpu.all_gather-multigpuallgatherop"><code>mgpu.all_gather</code> (multigpu::AllGatherOp)</h3>
<p><em>All-gather across all GPUs in a communicator</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.all_gather` $sendBuf `,` $recvBuf `,` $comm
              (`stream` $stream^ `:` type($stream))?
              attr-dict `:` type($sendBuf) `,` type($recvBuf) `,` type($comm)</code></pre>
<p>Each GPU sends <code>sendBuf</code> and every GPU receives the result in <code>recvBuf</code>. The types of the buffers must match. GPU i’s contribution appears at position i in the result.</p>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="operands">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>sendBuf</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>recvBuf</code></td>
<td>memref of any type values</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>comm</code></td>
<td>communicator handle</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>stream</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.all_reduce-multigpuallreduceop"><code>mgpu.all_reduce</code> (multigpu::AllReduceOp)</h3>
<p><em>All-reduce across all GPUs in a communication group</em></p>
<p>Each GPU sends <code>sendBuf</code> and receives the result in <code>recvBuf</code>. <code>reductionKind</code> is the reduction operation (for example,sum, prod, min, max). Send and receive buffers must have the same type, so a reduction with (sendBuf == recvBuf) is valid. All GPUs must<br />
call this operation.</p>
<p>Example:</p>
<pre class="mlir"><code>mgpu.all_reduce %send, %recv, %comm, sum
    : memref&lt;256xf32&gt;, memref&lt;256xf32&gt;, !mgpu.communicator</code></pre>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="attributes">Attributes:</h4>
<table>
<tr>
<th>
Attribute
</th>
<th>
MLIR Type
</th>
<th>
Description
</th>
</tr>
<tr>
<td>
<code>reductionKind</code>
</td>
<td>
::mlir::multigpu::ReductionKindAttr
</td>
<td>
Reduction operation for collective operations
</td>
</tr>
</table>
<h4 id="operands-1">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>sendBuf</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>recvBuf</code></td>
<td>memref of any type values</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>comm</code></td>
<td>communicator handle</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>stream</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.alloc-multigpuallocop"><code>mgpu.alloc</code> (multigpu::AllocOp)</h3>
<p><em>Allocate device memory</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.alloc` $device attr-dict `:` type($device) `-&gt;` type($ptr)</code></pre>
<p>Allocates memory for a device.</p>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-2">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>device</code></td>
<td>device handle</td>
</tr>
</tbody>
</table>
<h4 id="results">Results:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Result</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>ptr</code></td>
<td>memref of any type values</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.broadcast-multigpubroadcastop"><code>mgpu.broadcast</code> (multigpu::BroadcastOp)</h3>
<p><em>Broadcast from a GPU to all GPUs in a communicator</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.broadcast` $buf `,` $comm `,` $rootRank
              (`stream` $stream^ `:` type($stream))?
              attr-dict `:` type($buf) `,` type($comm)</code></pre>
<p>The <code>buf</code> is copied to every other GPU’s <code>buf</code>. All GPUs must call this op with the same root and buffer shape. This would usually be used to distribute model parameters, configuration data, or other data from a single source to all devices. The GPU’s buffer serves as the source, and all other GPUs receive the same data.</p>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-3">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>buf</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>comm</code></td>
<td>communicator handle</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>rootRank</code></td>
<td>index</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>stream</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.create_communicator-multigpucreatecommunicatorop"><code>mgpu.create_communicator</code> (multigpu::CreateCommunicatorOp)</h3>
<p><em>Create a communicator group for collective operations over a list of devices.</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.create_communicator` $devices attr-dict `:` type($devices) `-&gt;` type($comm)</code></pre>
<p>Establishes a communication group over a set of devices for collective operations (all_reduce, all_gather, broadcast, etc.). The ordering of devices determines each device’s index within the communicator (first device is index 0, second is index 1, etc.). All collective operations using this communicator must be called by all GPUs included in the operation.</p>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-4">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>devices</code></td>
<td>device handle</td>
</tr>
</tbody>
</table>
<h4 id="results-1">Results:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Result</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>comm</code></td>
<td>communicator handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.create_stream-multigpucreatestreamop"><code>mgpu.create_stream</code> (multigpu::CreateStreamOp)</h3>
<p><em>Create a new stream for a given device</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.create_stream` $device attr-dict `:` type($device) `-&gt;` type($stream)</code></pre>
<p>Creates an execution stream on a device. Streams are the mechanism provided to enable parallel execution. Each stream maintains its own queue of operations and executes them in order, but operations on different streams may run in parallel. In other words, each stream can be thought of as a separate thread of execution. The returned stream handle must be destroyed with <code>mgpu.destroy_stream</code> when no longer needed.</p>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-5">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>device</code></td>
<td>device handle</td>
</tr>
</tbody>
</table>
<h4 id="results-2">Results:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Result</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>stream</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.destroy_stream-multigpudestroystreamop"><code>mgpu.destroy_stream</code> (multigpu::DestroyStreamOp)</h3>
<p><em>Destroy a stream</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.destroy_stream` $stream attr-dict `:` type($stream)</code></pre>
<p>Releases the resources associated with a stream. All work enqueued on the stream should complete before destroying it. Destroying a stream with pending work is invalid.</p>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-6">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>stream</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.device_config-multigpudeviceconfigop"><code>mgpu.device_config</code> (multigpu::DeviceConfigOp)</h3>
<p><em>Symbolic declaration of the cluster configuration</em></p>
<p>Wraps <code>DeviceConfigAttr</code> to allow passes to do <code>SymbolTable</code> lookups. This op is just metadata.</p>
<p>Only one <code>mgpu.device_config</code> op should appear per module.</p>
<p>Example — 4 contiguous GPUs:</p>
<pre class="mlir"><code>mgpu.device_config @config #mgpu.device_config&lt;numDevices = 4&gt;</code></pre>
<p>Example — 4 GPUs with specific indices:</p>
<pre class="mlir"><code>mgpu.device_config @config
    #mgpu.device_config&lt;numDevices = 4, deviceIds = [0, 2, 4, 6]&gt;</code></pre>
<p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>HasParent&lt;::mlir::ModuleOp&gt;</code></p>
<p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code>, <code>Symbol</code></p>
<p>Effects: <code>MemoryEffects::Effect{}</code></p>
<h4 id="attributes-1">Attributes:</h4>
<table>
<tr>
<th>
Attribute
</th>
<th>
MLIR Type
</th>
<th>
Description
</th>
</tr>
<tr>
<td>
<code>sym_name</code>
</td>
<td>
::mlir::StringAttr
</td>
<td>
string attribute
</td>
</tr>
<tr>
<td>
<code>config</code>
</td>
<td>
::mlir::multigpu::DeviceConfigAttr
</td>
<td>
device config attribute
</td>
</tr>
</table>
<h3 id="mgpu.free-multigpufreeop"><code>mgpu.free</code> (multigpu::FreeOp)</h3>
<p><em>Free device memory</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.free` $ptr `,` $device attr-dict `:` type($ptr) `,` type($device)</code></pre>
<p>Releases memory allocated for a device. The pointer must have been allocated on the same device.</p>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-7">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>ptr</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>device</code></td>
<td>device handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.gather-multigpugatherop"><code>mgpu.gather</code> (multigpu::GatherOp)</h3>
<p><em>Concatenate device-local shards into a single destination</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.gather` $dst `,` $dstDevice `,` `[` $srcBufs `]` `,` `[` $devices `]`
              (`streams` `[` $streams^ `]` `:` type($streams))?
              `axis` `=` $axis
              attr-dict `:` type($dst) `,` type($dstDevice) `,`
              `[` type($srcBufs) `]` `,` `[` type($devices) `]`</code></pre>
<p>The opposite of <code>mgpu.scatter</code>. Copies shard buffers from multiple devices and combines them along <code>axis</code>. Supports uneven shards. Copies may be issued concurrently on the optional <code>streams</code>.</p>
<p>Example — gather 4x[256xf32] back into [1024xf32] on the host:</p>
<pre class="mlir"><code>mgpu.gather %hostBuf, %hostDev,
            %dBuf0, %dBuf1, %dBuf2, %dBuf3,
            %dev0, %dev1, %dev2, %dev3
    axis = 0
    : memref&lt;1024xf32&gt;, !mgpu.device,
      memref&lt;256xf32&gt;, memref&lt;256xf32&gt;, memref&lt;256xf32&gt;, memref&lt;256xf32&gt;,
      !mgpu.device, !mgpu.device, !mgpu.device, !mgpu.device</code></pre>
<p>Traits: <code>AttrSizedOperandSegments</code>, <code>RecursiveMemoryEffects</code></p>
<h4 id="attributes-2">Attributes:</h4>
<table>
<tr>
<th>
Attribute
</th>
<th>
MLIR Type
</th>
<th>
Description
</th>
</tr>
<tr>
<td>
<code>axis</code>
</td>
<td>
::mlir::IntegerAttr
</td>
<td>
index attribute
</td>
</tr>
</table>
<h4 id="operands-8">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>dst</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>dstDevice</code></td>
<td>device handle</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>srcBufs</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>devices</code></td>
<td>device handle</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>streams</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.get_device-multigpugetdeviceop"><code>mgpu.get_device</code> (multigpu::GetDeviceOp)</h3>
<p><em>Get a handle to a GPU device by index</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.get_device` $index attr-dict `:` type($device)</code></pre>
<p>Returns a handle to the device at the index. The index must be in the range [0, numDevices) specified by the <code>mgpu.device_config</code> attribute in the module.</p>
<p>Traits: <code>AlwaysSpeculatableImplTrait</code></p>
<p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p>
<p>Effects: <code>MemoryEffects::Effect{}</code></p>
<h4 id="operands-9">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>index</code></td>
<td>index</td>
</tr>
</tbody>
</table>
<h4 id="results-3">Results:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Result</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>device</code></td>
<td>device handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.launch-multigpulaunchop"><code>mgpu.launch</code> (multigpu::LaunchOp)</h3>
<p><em>Launch a kernel on a device</em></p>
<p>Launches a region as a kernel on the given device. The region body contains the kernel code that will execute on the GPU. Directly lowered to a GPU kernel.</p>
<p><code>grid</code> is the number of blocks in each dimension . <code>block</code> is the number of threads per block. The total number of threads launched is the product of grid and block.</p>
<p>If <code>stream</code> is provided the kernel is enqueued on that stream (async); otherwise it executes synchronously (blocking the host until completion).</p>
<p>Traits: <code>AttrSizedOperandSegments</code>, <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-10">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>device</code></td>
<td>device handle</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>grid</code></td>
<td>index</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>block</code></td>
<td>index</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>stream</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.memcpy-multigpumemcpyop"><code>mgpu.memcpy</code> (multigpu::MemcpyOp)</h3>
<p><em>Copy memory between devices</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.memcpy` $dst `,` $src `,` $dstDevice `,` $srcDevice
              (`stream` $stream^ `:` type($stream))?
              attr-dict `:` type($dst) `,` type($src) `,` type($dstDevice) `,` type($srcDevice)</code></pre>
<p>Copies the contents of <code>src</code> to <code>dst</code>. The source and destination memrefs should have the same element type and number of elements. Supports copying between different devices (peer-to-peer) or between host and device memory.</p>
<p>If <code>stream</code> is provided the copy is enqueued on that stream (async); otherwise it executes synchronously (blocking the host until the copy completes).</p>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-11">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>dst</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>src</code></td>
<td>memref of any type values</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>dstDevice</code></td>
<td>device handle</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>srcDevice</code></td>
<td>device handle</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>stream</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.recv-multigpurecvop"><code>mgpu.recv</code> (multigpu::RecvOp)</h3>
<p><em>Point-to-point receive</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.recv` $buf `,` $comm `,` $srcRank
              (`stream` $stream^ `:` type($stream))?
              attr-dict `:` type($buf) `,` type($comm)</code></pre>
<p>Receives data from the specified source GPU into <code>buf</code>. The buffer must have the same shape and element type as the buffer sent by the source GPU. If the send and receive operations are not paired correctly then it will cause deadlocks.</p>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-12">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>buf</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>comm</code></td>
<td>communicator handle</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>srcRank</code></td>
<td>index</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>stream</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.replicate-multigpureplicateop"><code>mgpu.replicate</code> (multigpu::ReplicateOp)</h3>
<p><em>The CPU copies a buffer to every GPU</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.replicate` $src `,` $srcDevice `,` `[` $devices `]` `,` `[` $dstBufs `]`
              (`streams` `[` $streams^ `]` `:` type($streams))?
              attr-dict `:` type($src) `,` type($srcDevice) `,`
              `[` type($devices) `]` `,` `[` type($dstBufs) `]`</code></pre>
<p>The CPU copies a buffer to all target GPUs. All destination buffers should have the same type as <code>src</code>. Copies may be issued concurrently on the optional <code>streams</code>.</p>
<p>Example — the CPU copies a [512xf32] weight buffer to 4 GPUs:</p>
<pre class="mlir"><code>mgpu.replicate %weights, %hostDev,
               %dev0, %dev1, %dev2, %dev3,
               %wBuf0, %wBuf1, %wBuf2, %wBuf3
    : memref&lt;512xf32&gt;, !mgpu.device,
      !mgpu.device, !mgpu.device, !mgpu.device, !mgpu.device,
      memref&lt;512xf32&gt;, memref&lt;512xf32&gt;, memref&lt;512xf32&gt;, memref&lt;512xf32&gt;</code></pre>
<p>Traits: <code>AttrSizedOperandSegments</code>, <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-13">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>src</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>srcDevice</code></td>
<td>device handle</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>devices</code></td>
<td>device handle</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>dstBufs</code></td>
<td>memref of any type values</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>streams</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.scatter-multigpuscatterop"><code>mgpu.scatter</code> (multigpu::ScatterOp)</h3>
<p><em>Distribute a buffer evenly across multiple GPUs</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.scatter` $src `,` $srcDevice `,` `[` $dstBufs `]` `,` `[` $devices `]`
              (`streams` `[` $streams^ `]` `:` type($streams))?
              `axis` `=` $axis
              attr-dict `:` type($src) `,` type($srcDevice) `,`
              `[` type($dstBufs) `]` `,` `[` type($devices) `]`</code></pre>
<p>Splits a buffer along <code>axis</code> into equal slices and copies them to the destination buffers on target GPUs. Requires the axis dimension to be divisible by the number of GPUs. Copies may be issued concurrently on the optional <code>streams</code>.</p>
<p>Example — split [1024xf32] from the host across 4 GPUs along axis 0:</p>
<pre class="mlir"><code>mgpu.scatter %hostBuf, %hostDev,
             %dBuf0, %dBuf1, %dBuf2, %dBuf3,
             %dev0, %dev1, %dev2, %dev3
    axis = 0
    : memref&lt;1024xf32&gt;, !mgpu.device,
      memref&lt;256xf32&gt;, memref&lt;256xf32&gt;, memref&lt;256xf32&gt;, memref&lt;256xf32&gt;,
      !mgpu.device, !mgpu.device, !mgpu.device, !mgpu.device</code></pre>
<p>Traits: <code>AttrSizedOperandSegments</code>, <code>RecursiveMemoryEffects</code></p>
<h4 id="attributes-3">Attributes:</h4>
<table>
<tr>
<th>
Attribute
</th>
<th>
MLIR Type
</th>
<th>
Description
</th>
</tr>
<tr>
<td>
<code>axis</code>
</td>
<td>
::mlir::IntegerAttr
</td>
<td>
index attribute
</td>
</tr>
</table>
<h4 id="operands-14">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>src</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>srcDevice</code></td>
<td>device handle</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>dstBufs</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>devices</code></td>
<td>device handle</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>streams</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.send-multigpusendop"><code>mgpu.send</code> (multigpu::SendOp)</h3>
<p><em>Point-to-point send</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.send` $buf `,` $comm `,` $dstRank
              (`stream` $stream^ `:` type($stream))?
              attr-dict `:` type($buf) `,` type($comm)</code></pre>
<p>Sends the contents of <code>buf</code> to the specified destination GPU within the communication group. The destination GPU must have a matching <code>mgpu.recv</code> operation. If the send and receive operations are not paired correctly then it will cause deadlocks.</p>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-15">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>buf</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>comm</code></td>
<td>communicator handle</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>dstRank</code></td>
<td>index</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>stream</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.sharded_alloc-multigpushardedallocop"><code>mgpu.sharded_alloc</code> (multigpu::ShardedAllocOp)</h3>
<p><em>Grouped allocation for memory allocation across multiple GPUs</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.sharded_alloc` $devices
              `logicalShape` `=` $logicalShape
              `shardAxis` `=` $shardAxis
              `shardKind` `=` $shardKind
              attr-dict `:` type($devices) `-&gt;` type($shards)</code></pre>
<p>Allocates one shard buffer per GPU.</p>
<p>Example — 4 shards of [1024x512xf32] along axis 0:</p>
<pre class="mlir"><code>%s0, %s1, %s2, %s3 = mgpu.sharded_alloc %dev0, %dev1, %dev2, %dev3
    logicalShape = [1024, 512] shardAxis = 0 shardKind = uniform
    : !mgpu.device, !mgpu.device, !mgpu.device, !mgpu.device
    -&gt; memref&lt;256x512xf32&gt;, memref&lt;256x512xf32&gt;,
       memref&lt;256x512xf32&gt;, memref&lt;256x512xf32&gt;</code></pre>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="attributes-4">Attributes:</h4>
<table>
<tr>
<th>
Attribute
</th>
<th>
MLIR Type
</th>
<th>
Description
</th>
</tr>
<tr>
<td>
<code>logicalShape</code>
</td>
<td>
::mlir::DenseI64ArrayAttr
</td>
<td>
i64 dense array attribute
</td>
</tr>
<tr>
<td>
<code>shardAxis</code>
</td>
<td>
::mlir::IntegerAttr
</td>
<td>
index attribute
</td>
</tr>
<tr>
<td>
<code>shardKind</code>
</td>
<td>
::mlir::multigpu::ShardKindAttr
</td>
<td>
Describes how a buffer is distributed(sharded) across GPUs
</td>
</tr>
</table>
<h4 id="operands-16">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>devices</code></td>
<td>device handle</td>
</tr>
</tbody>
</table>
<h4 id="results-4">Results:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Result</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>shards</code></td>
<td>memref of any type values</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.sharded_free-multigpushardedfreeop"><code>mgpu.sharded_free</code> (multigpu::ShardedFreeOp)</h3>
<p><em>Grouped free of GPU shard buffers</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.sharded_free` `[` $shards `]` `,` `[` $devices `]`
              attr-dict `:` `[` type($shards) `]` `,` `[` type($devices) `]`</code></pre>
<p>Free the buffers allocated on the group of GPUs. Each buffer in <code>shards</code> is freed on the corresponding GPU in <code>devices</code>.</p>
<p>Example:</p>
<pre class="mlir"><code>mgpu.sharded_free %s0, %s1, %s2, %s3, %dev0, %dev1, %dev2, %dev3
    : memref&lt;256x512xf32&gt;, memref&lt;256x512xf32&gt;,
      memref&lt;256x512xf32&gt;, memref&lt;256x512xf32&gt;,
      !mgpu.device, !mgpu.device, !mgpu.device, !mgpu.device</code></pre>
<p>Traits: <code>AttrSizedOperandSegments</code>, <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-17">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>shards</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>devices</code></td>
<td>device handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.stream_wait-multigpustreamwaitop"><code>mgpu.stream_wait</code> (multigpu::StreamWaitOp)</h3>
<p><em>Make one stream wait on another</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.stream_wait` $waitingStream `,` $waitOnStreams attr-dict `:`
              type($waitingStream) `,` type($waitOnStreams)</code></pre>
<p>Wait for all streams in <code>waitOnStreams</code> to complete. Work submitted to <code>waitingStream</code> after this op will not begin until all recorded work on the waited-on streams has completed. This enables more fine-grained synchronization rather than waiting for the entire GPU to complete. Streams may reside on different GPUs(and it’s almost expected for this dialect).</p>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-18">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>waitingStream</code></td>
<td>stream handle</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>waitOnStreams</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.sync_device-multigpusyncdeviceop"><code>mgpu.sync_device</code> (multigpu::SyncDeviceOp)</h3>
<p><em>Synchronize on a GPU</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.sync_device` $device attr-dict `:` type($device)</code></pre>
<p>Blocks the host until all work submitted to a GPU has completed. This operation waits for all work submitted on a GPU to complete. Use <code>mgpu.sync_stream</code> to synchronize individual streams.</p>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-19">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>device</code></td>
<td>device handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.sync_stream-multigpusyncstreamop"><code>mgpu.sync_stream</code> (multigpu::SyncStreamOp)</h3>
<p><em>Synchronize a single stream</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.sync_stream` $stream attr-dict `:` type($stream)</code></pre>
<p>Blocks the host until all work submitted to a stream has completed.</p>
<p>Traits: <code>RecursiveMemoryEffects</code></p>
<h4 id="operands-20">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>stream</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h3 id="mgpu.terminator-multigputerminatorop"><code>mgpu.terminator</code> (multigpu::TerminatorOp)</h3>
<p><em>Terminator for mgpu.launch regions</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.terminator` attr-dict</code></pre>
<p>A terminator for the region of a <code>mgpu.launch</code>. The region returns no values.</p>
<p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>HasParent&lt;LaunchOp&gt;</code>, <code>Terminator</code></p>
<p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p>
<p>Effects: <code>MemoryEffects::Effect{}</code></p>
<h3 id="mgpu.uneven_scatter-multigpuunevenscatterop"><code>mgpu.uneven_scatter</code> (multigpu::UnevenScatterOp)</h3>
<p><em>Distribute a buffer across devices with asymmetric slice sizes</em></p>
<p>Syntax:</p>
<pre><code>operation ::= `mgpu.uneven_scatter` $src `,` $srcDevice `,` `[` $dstBufs `]` `,` `[` $devices `]`
              (`streams` `[` $streams^ `]` `:` type($streams))?
              `sliceSizes` `=` $sliceSizes `axis` `=` $axis
              attr-dict `:` type($src) `,` type($srcDevice) `,`
              `[` type($dstBufs) `]` `,` `[` type($devices) `]`</code></pre>
<p>Similar to <code>mgpu.scatter</code>, but the size of data to be distributed to each GPU is specified. Copies may be issued concurrently on the optional <code>streams</code>.</p>
<p>Example — partition [100xf32] across 3 GPUs as [34, 33, 33]:</p>
<pre class="mlir"><code>mgpu.uneven_scatter %src, %srcDev,
                    %dBuf0, %dBuf1, %dBuf2,
                    %dev0, %dev1, %dev2
    sliceSizes = [34, 33, 33] axis = 0
    : memref&lt;100xf32&gt;, !mgpu.device,
      memref&lt;34xf32&gt;, memref&lt;33xf32&gt;, memref&lt;33xf32&gt;,
      !mgpu.device, !mgpu.device, !mgpu.device</code></pre>
<p>Traits: <code>AttrSizedOperandSegments</code>, <code>RecursiveMemoryEffects</code></p>
<h4 id="attributes-5">Attributes:</h4>
<table>
<tr>
<th>
Attribute
</th>
<th>
MLIR Type
</th>
<th>
Description
</th>
</tr>
<tr>
<td>
<code>sliceSizes</code>
</td>
<td>
::mlir::DenseI64ArrayAttr
</td>
<td>
i64 dense array attribute
</td>
</tr>
<tr>
<td>
<code>axis</code>
</td>
<td>
::mlir::IntegerAttr
</td>
<td>
index attribute
</td>
</tr>
</table>
<h4 id="operands-21">Operands:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Operand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>src</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>srcDevice</code></td>
<td>device handle</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>dstBufs</code></td>
<td>memref of any type values</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>devices</code></td>
<td>device handle</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>streams</code></td>
<td>stream handle</td>
</tr>
</tbody>
</table>
<h2 id="attributes-6">Attributes</h2>
<h3 id="deviceconfigattr">DeviceConfigAttr</h3>
<p>Compile-time GPU configuration for the module (at the moment, just the number of GPUs and their IDs)</p>
<p>Syntax:</p>
<pre><code>#mgpu.device_config&lt;
  uint32_t,   # numDevices
  DenseI32ArrayAttr   # deviceIds
&gt;</code></pre>
<p>A structured attribute recording the GPU cluster configuration known at compile time.</p>
<p>Fields: - <code>numDevices</code>: The total count of GPUs available. - <code>deviceIds</code>: Optional explicit physical device indices for non-contiguous sets (e.g., [0, 2, 4, 6]).</p>
<p>Passes retrieve this info through the <code>mgpu.device_config</code> symbol op.</p>
<p>Example — 4 contiguous GPUs:</p>
<pre class="mlir"><code>#mgpu.device_config&lt;numDevices = 4&gt;</code></pre>
<p>Example — 4 GPUs with specific indices:</p>
<pre class="mlir"><code>#mgpu.device_config&lt;numDevices = 4, deviceIds = [0, 2, 4, 6]&gt;</code></pre>
<h4 id="parameters">Parameters:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Parameter</th>
<th style="text-align: center;">C++ type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">numDevices</td>
<td style="text-align: center;"><code>uint32_t</code></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: center;">deviceIds</td>
<td style="text-align: center;"><code>DenseI32ArrayAttr</code></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="types">Types</h2>
<h3 id="communicatortype">CommunicatorType</h3>
<p>A group of devices that can be used for collective communication operations</p>
<p>Syntax: <code>!mgpu.communicator</code></p>
<h3 id="devicetype">DeviceType</h3>
<p>An opaque handle to a GPU device</p>
<p>Syntax: <code>!mgpu.device</code></p>
<h3 id="streamtype">StreamType</h3>
<p>An opaque handle to a device stream</p>
<p>Syntax: <code>!mgpu.stream</code></p>
<h2 id="enums">Enums</h2>
<h3 id="reductionkind">ReductionKind</h3>
<p>Reduction operation for collective operations</p>
<h4 id="cases">Cases:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Symbol</th>
<th style="text-align: center;">Value</th>
<th>String</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Sum</td>
<td style="text-align: center;"><code>0</code></td>
<td>sum</td>
</tr>
<tr class="even">
<td style="text-align: center;">Prod</td>
<td style="text-align: center;"><code>1</code></td>
<td>prod</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Min</td>
<td style="text-align: center;"><code>2</code></td>
<td>min</td>
</tr>
<tr class="even">
<td style="text-align: center;">Max</td>
<td style="text-align: center;"><code>3</code></td>
<td>max</td>
</tr>
</tbody>
</table>
<h3 id="shardkind">ShardKind</h3>
<p>Describes how a buffer is distributed(sharded) across GPUs</p>
<h4 id="cases-1">Cases:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Symbol</th>
<th style="text-align: center;">Value</th>
<th>String</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Uniform</td>
<td style="text-align: center;"><code>0</code></td>
<td>uniform</td>
</tr>
<tr class="even">
<td style="text-align: center;">Uneven</td>
<td style="text-align: center;"><code>1</code></td>
<td>uneven</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Replicate</td>
<td style="text-align: center;"><code>2</code></td>
<td>replicate</td>
</tr>
</tbody>
</table>
</body>
</html>
